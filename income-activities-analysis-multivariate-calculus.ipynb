{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jocelyndumlao/income-activities-analysis-multivariate-calculus?scriptVersionId=145032017\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b> Import Libraries</p></div>\n","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install fasteda","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:48:26.929099Z","iopub.execute_input":"2023-10-03T00:48:26.929477Z","iopub.status.idle":"2023-10-03T00:48:40.325554Z","shell.execute_reply.started":"2023-10-03T00:48:26.929443Z","shell.execute_reply":"2023-10-03T00:48:40.324355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom fasteda import fast_eda\nfrom sympy import symbols, diff, hessian, Matrix, Eq, solve\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:48:44.233517Z","iopub.execute_input":"2023-10-03T00:48:44.233893Z","iopub.status.idle":"2023-10-03T00:48:45.607338Z","shell.execute_reply.started":"2023-10-03T00:48:44.233862Z","shell.execute_reply":"2023-10-03T00:48:45.606525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Load the Data</p></div>\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/income-activities/dbo-incomes.csv', encoding='latin1')\ndf.head().style.set_properties(**{\"background-color\": \"#FF8C00\",\"color\":\"white\",\"border\": \"1.5px solid Black\"})","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:48:50.723899Z","iopub.execute_input":"2023-10-03T00:48:50.724375Z","iopub.status.idle":"2023-10-03T00:48:50.850683Z","shell.execute_reply.started":"2023-10-03T00:48:50.724347Z","shell.execute_reply":"2023-10-03T00:48:50.849641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Fast Exploratory Data Analysis</p></div>","metadata":{}},{"cell_type":"code","source":"# 'df' is a pandas DataFrame\nnumeric_df = df.select_dtypes(include=['number'])  # Select only numeric columns\n\nfast_eda(numeric_df)  # Use the numeric columns for EDA\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:00.403693Z","iopub.execute_input":"2023-10-03T00:49:00.40419Z","iopub.status.idle":"2023-10-03T00:49:06.61795Z","shell.execute_reply.started":"2023-10-03T00:49:00.404159Z","shell.execute_reply":"2023-10-03T00:49:06.616724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Multivariate Function</p></div>\n\n<p style=\"color: darkblue; font-size: 16px;\">\nA multivariate function is a mathematical function that depends on more than one variable. In other words, it takes multiple inputs, and its output is determined by the values of these variables. For instance, in a function $f(x,y)$, both $x$ and $y$ are variables influencing the result.</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 15px;\">here's an equation for illustration and explanation.</p>\n\n\n#### Multivariate Function:\n\n $f(x, y) = 2x + 3y$\n\n#### ðŸ”˜ Partial Derivatives:\n\n  * Partial derivative with respect to $ x :\n\\frac{\\partial f}{\\partial x} = 2 $\n\n  * Partial derivative with respect to $ y:\n \\frac{\\partial f}{\\partial y} = 3 $\n\n#### ðŸ”˜ Gradient Descent Optimization:\n\n  * We used gradient descent with a learning rate of 0.01 to find the optimal values of $x$ and $y$. After 1000 iterations, we found the optimal values to be $ x â‰ˆ 0$  and $y â‰ˆ 0$.\n\n#### ðŸ”˜ Critical Points and Hessian Matrix: \n\n  * There are no critical points because the partial derivatives are constant, and hence, there are no points where both are simultaneously 0.\n\n#### ðŸ”˜ Second-Order Taylor Expansion\n  * Around the point $(1,2)$, the second-order Taylor expansion is:\n$f(x,y)â‰ˆ8+2(xâˆ’1)+3(yâˆ’2)$\n\n#### ðŸ”˜ Visualization:\n  * The function \\(f(x, y) = 2x + 3y\\) represents a plane in three-dimensional space. The slope in the \\(x\\)-direction is \\(2\\) and in the \\(y\\)-direction is \\(3\\). This means that for every unit increase in \\(x\\), the function increases by \\(2\\) units, and for every unit increase in \\(y\\), the function increases by \\(3\\) units.","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 16px;\">Define a function that represents a multivariate function using some combination of the input features.\n</p>\n\n#### We'll define a multivariate function. For simplicity, let's use a linear combination of two features, `Income_Month` and `IncomeValue`:\n","metadata":{}},{"cell_type":"code","source":"def multivariate_function(x, y):\n    return 2*x + 3*y","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:33.759644Z","iopub.execute_input":"2023-10-03T00:49:33.760025Z","iopub.status.idle":"2023-10-03T00:49:33.764456Z","shell.execute_reply.started":"2023-10-03T00:49:33.759996Z","shell.execute_reply":"2023-10-03T00:49:33.763477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 16px;\">Now, let's calculate partial derivatives using sympy:</p>\n\n","metadata":{}},{"cell_type":"code","source":"from sympy import symbols, diff\n\nx, y = symbols('x y')\nf = 2*x + 3*y\n\n# Partial derivatives\ndf_dx = diff(f, x)\ndf_dy = diff(f, y)\n\nprint(\"Partial derivative with respect to x:\", df_dx)\nprint(\"Partial derivative with respect to y:\", df_dy)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:39.931218Z","iopub.execute_input":"2023-10-03T00:49:39.931719Z","iopub.status.idle":"2023-10-03T00:49:40.005598Z","shell.execute_reply.started":"2023-10-03T00:49:39.931677Z","shell.execute_reply":"2023-10-03T00:49:40.004304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 16px;\">Next, we'll calculate the Jacobian and Hessian matrices:</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"color:#74088b;\">Hessian Matrix:</h3>\n<p style=\"color: brown; font-size: 16px;\">Definition: The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function.</p>\n<p style=\"color: brown; font-size: 16px;\">Use: It provides information about the curvature and behavior of a function around a critical point.</p>\n\n<p style=\"color: brown; font-size: 16px;\">Significance: Positive-definite Hessian implies a local minimum, negative-definite implies a local maximum, and indefinite implies a saddle point.\n</p>\n\n\n<h3 style=\"color: #2ed00b;\">Jacobian Matrix:</h3>\n<p style=\"color: orange; font-size: 16px;\">Definition: The Jacobian matrix is a matrix of first-order partial derivatives of a vector-valued function.</p>\n\n<p style=\"color: orange; font-size: 16px;\">Use: It describes the rate at which a multivariate function changes with respect to its inputs.</p>\n\n<p style=\"color: orange; font-size: 16px;\">Significance: Useful in various fields including optimization, physics, and machine learning for tasks like gradient-based optimization and computing the transformation matrix in robotics.</p>\n\n<p style=\"color: darkblue; font-size: 16px;\">These matrices play crucial roles in calculus, optimization, and various areas of mathematics and its applications.</p>\n\n","metadata":{}},{"cell_type":"code","source":"from sympy import hessian, Matrix\n\n# Hessian matrix\nhessian_matrix = hessian(f, (x, y))\njacobian_matrix = Matrix([df_dx, df_dy])\n\nprint(\"Hessian Matrix:\")\nprint(hessian_matrix)\n\nprint(\"Jacobian Matrix:\")\nprint(jacobian_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:47.438571Z","iopub.execute_input":"2023-10-03T00:49:47.438942Z","iopub.status.idle":"2023-10-03T00:49:47.446095Z","shell.execute_reply.started":"2023-10-03T00:49:47.438917Z","shell.execute_reply":"2023-10-03T00:49:47.445014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 16px;\">Now, let's create a plot for visualization. Since we're dealing with a 2D function, we can create a contour plot:</p>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nx_vals = np.linspace(-10, 10, 100)\ny_vals = np.linspace(-10, 10, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = multivariate_function(X, Y)\n\n# Create contour plot\nplt.figure(figsize=(8, 6))\ncontour = plt.contourf(X, Y, Z, cmap='viridis')\nplt.colorbar(contour)\nplt.xlabel('Income_Month', fontsize = 12, fontweight = 'bold', color = 'magenta')\nplt.ylabel('IncomeValue', fontsize = 12, fontweight = 'bold', color = 'magenta')\nplt.title('Contour Plot of Multivariate Function', fontsize = 14, fontweight = 'bold', color = 'darkblue')\nplt.savefig('Contour Plot of Multivariate Function.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:52.686445Z","iopub.execute_input":"2023-10-03T00:49:52.686795Z","iopub.status.idle":"2023-10-03T00:49:53.231819Z","shell.execute_reply.started":"2023-10-03T00:49:52.686749Z","shell.execute_reply":"2023-10-03T00:49:53.230734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Partial derivative with respect to x measures how the function changes with respect to changes in Income_Month.\n* Partial derivative with respect to y measures how the function changes with respect to changes in IncomeValue.\n* The Jacobian matrix contains the partial derivatives, which can be used in gradient descent optimization.\n* The Hessian matrix provides information about curvature and is important in higher-dimensional optimization problems.\n","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Optimization using Gradient Descent</p></div>\n\nðŸ”˜ We can demonstrate how to use the partial derivatives to perform gradient descent for optimizing a particular objective function.","metadata":{}},{"cell_type":"code","source":"# Define a learning rate and initial guess for the parameters\nlearning_rate = 0.01\ninitial_x = 0\ninitial_y = 0\n\n# Perform gradient descent\nfor _ in range(1000):\n    gradient_x = df_dx.evalf(subs={x: initial_x, y: initial_y})\n    gradient_y = df_dy.evalf(subs={x: initial_x, y: initial_y})\n    initial_x = initial_x - learning_rate * gradient_x\n    initial_y = initial_y - learning_rate * gradient_y\n\n# The final (x, y) values will be the optimal solution\nprint(\"Optimal (x, y):\", (initial_x, initial_y))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:50:01.728543Z","iopub.execute_input":"2023-10-03T00:50:01.728946Z","iopub.status.idle":"2023-10-03T00:50:01.81868Z","shell.execute_reply.started":"2023-10-03T00:50:01.728914Z","shell.execute_reply":"2023-10-03T00:50:01.817498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Critical Points and Optimization</p></div>\n\nðŸ”˜ We can find critical points (where gradients are zero) and analyze their nature (minima, maxima, or saddle points) using the Hessian matrix.","metadata":{}},{"cell_type":"code","source":"from sympy import Eq, solve\n\n# we have a function and its Hessian matrix defined\ncritical_points = solve([Eq(df_dx, 0), Eq(df_dy, 0)], (x, y))\n\nfor point in critical_points:\n    hessian_at_point = hessian_matrix.evalf(subs={x: point[x], y: point[y]})\n    eigenvalues = hessian_at_point.eigenvals()\n    print(f\"Critical Point: {point}\")\n    print(f\"Hessian Matrix: {hessian_at_point}\")\n    print(f\"Eigenvalues: {eigenvalues}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:50:07.648356Z","iopub.execute_input":"2023-10-03T00:50:07.649241Z","iopub.status.idle":"2023-10-03T00:50:07.655155Z","shell.execute_reply.started":"2023-10-03T00:50:07.649208Z","shell.execute_reply":"2023-10-03T00:50:07.653705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magents;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Second-Order Taylor Expansion</p></div>\n\nðŸ”˜ We can use the Hessian matrix to perform a second-order Taylor expansion around a point to approximate the behavior of the function.\n","metadata":{}},{"cell_type":"code","source":"# Choose a point for expansion\npoint = (1, 2)\n\n# Calculate Taylor expansion\ntaylor_expansion = f.series(x, y, n=2).removeO().expand().subs({x: point[0], y: point[1]})\nprint(f\"Second-Order Taylor Expansion at {point}: {taylor_expansion}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:50:12.73962Z","iopub.execute_input":"2023-10-03T00:50:12.739973Z","iopub.status.idle":"2023-10-03T00:50:12.763826Z","shell.execute_reply.started":"2023-10-03T00:50:12.739947Z","shell.execute_reply":"2023-10-03T00:50:12.762521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`from sympy import symbols, diff, hessian, Matrix, Eq, solve`\n\nexplanations for each of the libraries:\n\nðŸŸ  `from sympy import symbols`:\n\n* `symbols` is a function from the `sympy` library, which is used for creating symbolic variables in Python. These variables can be used to represent mathematical symbols like $x$, $y$, etc., allowing for symbolic mathematics operations.\n\nðŸŸ  `diff`:\n\n* The diff function is used for symbolic differentiation in sympy. It calculates the derivative of a given expression with respect to a specified variable. For example, diff($f$, $x$) computes the partial derivative of $f$ with respect to $x$.\n\nðŸŸ  `hessian`:\n\n* The `hessian` function in `sympy` is used to calculate the Hessian matrix of a multivariable function. The Hessian matrix contains second-order partial derivatives and provides information about the curvature and behavior of the function around critical points.\n\nðŸŸ  `Matrix`:\n\n* `Matrix` is a class in `sympy` that represents matrices in symbolic form. It allows you to perform various matrix operations, such as addition, subtraction, multiplication, and finding determinants.\n\nðŸŸ  `Eq`:\n\n* `Eq` is a class in `sympy` that represents a mathematical equality. It is used to set up and solve equations symbolically. For example, `Eq(a,b)` represents the equation $a=b$.\n\nðŸŸ  `solve`:\n\n* The `solve` function in `sympy` is used to find solutions to algebraic equations or systems of equations. It takes an equation or a list of equations as input and attempts to find values of the variables that satisfy the equations.\n\nThese libraries and functions from `sympy` are incredibly useful for symbolic mathematics operations, making them valuable tools for tasks like symbolic differentiation, solving equations, and working with matrices. They are commonly used in fields like mathematics, physics, engineering, and computer science.","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color: green; font-size: 16px;\">The multivariate function is a linear combination of Income_Month and IncomeValue. The partial derivatives, Jacobian, and Hessian matrices will be calculated accordingly.</p>\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sympy import symbols, diff, hessian, Matrix, Eq, solve\n\n# Define symbols\nx, y = symbols('x y')\n\n# Define the multivariate function\ndef multivariate_function(x, y):\n    return 2*x + 3*y\n\n# Calculate partial derivatives\ndf_dx = diff(multivariate_function(x, y), x)\ndf_dy = diff(multivariate_function(x, y), y)\n\n# Calculate Hessian matrix\nhessian_matrix = hessian(multivariate_function(x, y), (x, y))\njacobian_matrix = Matrix([df_dx, df_dy])\n\n# Print partial derivatives and Hessian matrix\nprint(\"Partial derivative with respect to x:\", df_dx)\nprint(\"Partial derivative with respect to y:\", df_dy)\n\nprint(\"\\nJacobian Matrix:\")\nprint(jacobian_matrix)\n\nprint(\"\\nHessian Matrix:\")\nprint(hessian_matrix)\n\n# Perform optimization using Gradient Descent\nlearning_rate = 0.01\ninitial_x = 0\ninitial_y = 0\n\nfor _ in range(1000):\n    gradient_x = df_dx.evalf(subs={x: initial_x, y: initial_y})\n    gradient_y = df_dy.evalf(subs={x: initial_x, y: initial_y})\n    initial_x = initial_x - learning_rate * gradient_x\n    initial_y = initial_y - learning_rate * gradient_y\n\nprint(\"\\nOptimal (x, y):\", (initial_x, initial_y))\n\n# Find critical points and analyze their nature\ncritical_points = solve([Eq(df_dx, 0), Eq(df_dy, 0)], (x, y))\n\nfor point in critical_points:\n    hessian_at_point = hessian_matrix.evalf(subs={x: point[x], y: point[y]})\n    eigenvalues = hessian_at_point.eigenvals()\n    print(f\"\\nCritical Point: {point}\")\n    print(f\"Hessian Matrix: {hessian_at_point}\")\n    print(f\"Eigenvalues: {eigenvalues}\")\n\n# Second-Order Taylor Expansion\npoint = (1, 2)\ntaylor_expansion = multivariate_function(x, y).series(x, y, n=2).removeO().expand().subs({x: point[0], y: point[1]})\nprint(f\"\\nSecond-Order Taylor Expansion at {point}: {taylor_expansion}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:50:25.997496Z","iopub.execute_input":"2023-10-03T00:50:25.997835Z","iopub.status.idle":"2023-10-03T00:50:26.095305Z","shell.execute_reply.started":"2023-10-03T00:50:25.99781Z","shell.execute_reply":"2023-10-03T00:50:26.094113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the function\ndef f(x, y):\n    return x**2 + y**2\n\n# Gradient Descent Optimization\nlearning_rate = 0.1\niterations = 20\ninitial_x = 3\ninitial_y = 3\n\nx_vals = [initial_x]\ny_vals = [initial_y]\n\nfor _ in range(iterations):\n    gradient_x = 2 * initial_x\n    gradient_y = 2 * initial_y\n    initial_x = initial_x - learning_rate * gradient_x\n    initial_y = initial_y - learning_rate * gradient_y\n    x_vals.append(initial_x)\n    y_vals.append(initial_y)\n\n# Create a meshgrid\nx_vals_mesh, y_vals_mesh = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n\n# Evaluate the function on the meshgrid\nz_vals = f(x_vals_mesh, y_vals_mesh)\n\n# Plot the optimization path\nplt.figure(figsize=(8, 6))\nplt.contourf(x_vals_mesh, y_vals_mesh, z_vals, levels=20, cmap='viridis')\nplt.colorbar()\nplt.plot(x_vals, y_vals, 'ro-')\nplt.title('Gradient Descent Optimization', fontsize = 14, fontweight = 'bold', color = 'darkblue')\nplt.xlabel('x', fontsize = 12, fontweight = 'bold', color = 'magenta')\nplt.ylabel('y', fontsize = 12, fontweight = 'bold', color = 'magenta')\nplt.savefig('Gradient Descent Optimization.png')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-03T00:50:34.703297Z","iopub.execute_input":"2023-10-03T00:50:34.703637Z","iopub.status.idle":"2023-10-03T00:50:35.225012Z","shell.execute_reply.started":"2023-10-03T00:50:34.703611Z","shell.execute_reply":"2023-10-03T00:50:35.224038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Critical Points and Optimization\nx_vals = np.linspace(-5, 5, 400)\ny_vals = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = f(X, Y)\n\n# Find critical point\nmin_value = np.min(Z)\nmin_index = np.unravel_index(np.argmin(Z, axis=None), Z.shape)\ncritical_x, critical_y = x_vals[min_index[1]], y_vals[min_index[0]]\n\n# Plot the function and critical point\nplt.figure(figsize=(8, 6))\ncontour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\nplt.colorbar(contour)\nplt.plot(critical_x, critical_y, 'ro')\nplt.title(f'Critical Point at ({critical_x:.2f}, {critical_y:.2f})', fontsize = 14, fontweight = 'bold', color = 'darkblue')\nplt.xlabel('x', fontsize = 12, fontweight = 'bold', color = 'darkred')\nplt.ylabel('y', fontsize = 12, fontweight = 'bold', color = 'darkred')\nplt.savefig('Critical Point.png')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-03T00:50:46.370923Z","iopub.execute_input":"2023-10-03T00:50:46.37128Z","iopub.status.idle":"2023-10-03T00:50:46.9494Z","shell.execute_reply.started":"2023-10-03T00:50:46.371251Z","shell.execute_reply":"2023-10-03T00:50:46.948475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Second-Order Taylor Expansion</p></div>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sympy import symbols, diff\n\n# Define the function\ndef f(x, y):\n    return x**2 + y**2\n\n# Second-Order Taylor Expansion\npoint = (1, 2)\nx, y = symbols('x y')\ntaylor_expansion = f(x, y).series(x, y, n=2).removeO().expand().subs({x: point[0], y: point[1]})\n\n# Define a meshgrid\nx_vals = np.linspace(-2, 4, 400)\ny_vals = np.linspace(-2, 5, 400)\nX, Y = np.meshgrid(x_vals, y_vals)\n\n\n# Calculate the function values and the Taylor approximation\nZ = f(X, Y)\nZ_taylor = np.array(Z, dtype=np.float64)\n\n# Plot the function and its second-order Taylor approximation\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\nplt.colorbar(contour)\ncontour_taylor = plt.contour(X, Y, Z_taylor, levels=20, cmap='autumn')\nplt.colorbar(contour_taylor)\nplt.title(f'Second-Order Taylor Expansion at ({point[0]}, {point[1]})', fontsize = 14, fontweight = 'bold', color = 'darkblue')\nplt.xlabel('x', fontsize = 12, fontweight = 'bold', color = 'darkgreen')\nplt.ylabel('y', fontsize = 12, fontweight = 'bold', color = 'darkgreen')\nplt.savefig('Second-Order Taylor Expansion.png')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-03T00:50:53.240239Z","iopub.execute_input":"2023-10-03T00:50:53.240574Z","iopub.status.idle":"2023-10-03T00:50:54.233674Z","shell.execute_reply.started":"2023-10-03T00:50:53.240548Z","shell.execute_reply":"2023-10-03T00:50:54.232545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:magenta;display:inline-block;border-radius:5px;background-color:#F0E68C;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:magenta;overflow:hidden;font-size:90%;letter-spacing:0.5px;margin:0\"><b> </b>Third-Order Taylor Expansion</p></div>\n","metadata":{}},{"cell_type":"markdown","source":"#### The given equation represents a third-order Taylor expansion of a function $f(x, y)$ around a specific point $(a, b)$ is given by:\n\n$ f(x,y) \\approx f(a,b) + \\frac{{\\partial f}}{{\\partial x}}(a,b) \\cdot (x-a) + \\frac{{\\partial f}}{{\\partial y}}(a,b) \\cdot (y-b) + \\frac{{1}}{{2!}} \\left( \\frac{{\\partial^2 f}}{{\\partial x^2}}(a,b) \\cdot (x-a)^2 + 2 \\cdot \\frac{{\\partial^2 f}}{{\\partial x \\partial y}}(a,b) \\cdot (x-a)(y-b) + \\frac{{\\partial^2 f}}{{\\partial y^2}}(a,b) \\cdot (y-b)^2 \\right) + \\frac{{1}}{{3!}} \\left( \\frac{{\\partial^3 f}}{{\\partial x^3}}(a,b) \\cdot (x-a)^3 + 3 \\cdot \\frac{{\\partial^3 f}}{{\\partial x^2 \\partial y}}(a,b) \\cdot (x-a)^2 (y-b) + 3 \\cdot \\frac{{\\partial^3 f}}{{\\partial x \\partial y^2}}(a,b) \\cdot (x-a)(y-b)^2 + \\frac{{\\partial^3 f}}{{\\partial y^3}}(a,b) \\cdot (y-b)^3 \\right) $\n\n#### This expansion involves first-order partial derivatives $\\frac{{\\partial f}}{{\\partial x}}, \\frac{{\\partial f}}{{\\partial y}}$, second-order mixed partial derivatives $\\frac{{\\partial^2 f}}{{\\partial x^2 \\partial y}}, \\frac{{\\partial^2 f}}{{\\partial x \\partial y^2}}$, and third-order partial derivatives $\\frac{{\\partial^3 f}}{{\\partial x^3}}, \\frac{{\\partial^3 f}}{{\\partial x^2 \\partial y}}, \\frac{{\\partial^3 f}}{{\\partial x \\partial y^2}}, \\frac{{\\partial^3 f}}{{\\partial y^3}}$, etc.\n","metadata":{}},{"cell_type":"markdown","source":"#### Let's break down the components of the equation:\n\n##### ðŸŸ¢ $(f(x, y) \\approx f(a, b))$: This term represents the value of the function at the point $(a, b)$. It serves as the starting point for the approximation.\n\n##### ðŸŸ¢ $\\frac{{\\partial f}}{{\\partial x}}(a, b) \\cdot (x - a)$: This term involves the partial derivative of $(f$) with respect to $(x$) at the point $(a, b)$. It measures how the function changes with respect to $(x$) at this point. Multiplying this derivative by $(x - a)$ accounts for changes in $(x$) relative to $(a$).\n\n##### ðŸŸ¢ $\\frac{{\\partial f}}{{\\partial y}}(a, b) \\cdot (y - b)$: Similar to the previous term, this involves the partial derivative of $(f$) with respect to $(y$) at the point $(a, b)$. It measures how the function changes with respect to $(y$) at this point. Multiplying this derivative by $(y - b)$ accounts for changes in $(y$) relative to $(b$).\n\n##### ðŸŸ¢ $\\frac{1}{{2!}}$: This term represents the factorial of 2, which is 2. It's included to account for the second-order derivatives.\n\n##### ðŸŸ¢ The terms within the second set of parentheses $(...$) involve second-order partial derivatives and mixed partial derivatives. These terms account for changes in both $(x$) and $(y$) relative to $(a$) and $(b$) respectively.\n\n##### ðŸŸ¢ $\\frac{1}{{3!}}$: This term represents the factorial of 3, which is 6. It's included to account for the third-order derivatives.\n\n##### ðŸŸ¢ The terms within the third set of parentheses $(...$) involve third-order partial derivatives. These terms account for changes in both \\(x\\) and \\(y\\) relative to \\(a\\) and \\(b\\) respectively.\n\n##### In summary, this third-order Taylor expansion incorporates first-order, second-order, and third-order partial derivatives to provide a detailed approximation of the function near the point $(a, b)$. This expansion allows for a more precise estimation of the function's behavior in the vicinity of the chosen point.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sympy import symbols, diff\n\n# Define the function\ndef f(x, y):\n    return x**2 + y**2\n\n# Define the point for the Taylor expansion\npoint = (1, 2)\nx, y = symbols('x y')\n\n# Calculate first-order partial derivatives\ndf_dx = diff(f(x, y), x)\ndf_dy = diff(f(x, y), y)\n\n# Calculate second-order mixed partial derivatives\ndf_dxdy = diff(df_dx, y)\ndf_dydx = diff(df_dy, x)\n\n# Calculate third-order partial derivatives\ndf_dxdxdy = diff(df_dxdy, x)\ndf_dxdydx = diff(df_dydx, y)\ndf_dydydx = diff(df_dydx, y)\n\n# Compute the third-order Taylor expansion\ntaylor_expansion = f(x, y) + df_dx*(x - point[0]) + df_dy*(y - point[1]) + \\\n    (1/2)*df_dxdxdy*(x - point[0])**2 + df_dxdydx*(x - point[0])*(y - point[1]) + (1/2)*df_dydydx*(y - point[1])**2\n\n# Define a meshgrid for plotting\nx_vals = np.linspace(-2, 4, 400)\ny_vals = np.linspace(-2, 5, 400)\nX, Y = np.meshgrid(x_vals, y_vals)\n\n# Calculate the function values and Taylor approximation\n#Z_taylor = np.array(Z_taylor, dtype=np.float64)\n\nZ = f(X, Y)\nZ_taylor = np.array([[taylor_expansion.evalf(subs={x: x_val, y: y_val}) for x_val, y_val in zip(row_x, row_y)] for row_x, row_y in zip(X, Y)])\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-03T00:51:04.886984Z","iopub.execute_input":"2023-10-03T00:51:04.887324Z","iopub.status.idle":"2023-10-03T00:52:32.811492Z","shell.execute_reply.started":"2023-10-03T00:51:04.887298Z","shell.execute_reply":"2023-10-03T00:52:32.810301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Z_taylor = np.array(Z_taylor, dtype=np.float64)\n\n# Plot the function and its third-order Taylor approximation\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\nplt.colorbar(contour)\ncontour_taylor = plt.contour(X, Y, Z_taylor, levels=20, cmap='autumn')\nplt.colorbar(contour_taylor)\nplt.title(f'Third-Order Taylor Expansion at ({point[0]}, {point[1]})', fontsize = 14, fontweight = 'bold', color = 'darkblue')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig('Third-Order Taylor Expansion.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:54:05.326106Z","iopub.execute_input":"2023-10-03T00:54:05.32649Z","iopub.status.idle":"2023-10-03T00:54:06.632654Z","shell.execute_reply.started":"2023-10-03T00:54:05.326458Z","shell.execute_reply":"2023-10-03T00:54:06.631855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\"Your positive feedback and upvote mean a lot! It motivates me to create more valuable content and helps others discover it too. Let's build a thriving community of knowledge-sharing. Thank you for your support! ðŸ˜Š\"</div>","metadata":{}}]}